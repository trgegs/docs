---
title: "Custom monitoring"
---

BigAnimal has the ability to expose metrics and logs. To enable this on your cluster, contact [BigAnimal Support](/biganimal/release/overview/support).


## Metrics

If you are using `customer metrics` you will be able to access metrics in a prometheus format. We have provided some [example metrics](/biganimal/latest/using_cluster/05_monitoring_and_logging/custom_monitoring/example_metrics/) to help get you started.

If private networking is configured for the metrics, you will likely need to configure network peering:

* [AWS](/biganimal/release/using_cluster/02_connecting_your_cluster/01_connecting_from_azure/02_virtual_network_peering)
* [Azure](product_docs/docs/biganimal/release/using_cluster/02_connecting_your_cluster/01_connecting_from_azure/02_virtual_network_peering)

### Patterns for accessing metrics

A common pattern for metric shipping is to have the vendor-supplied agent scrape the metrics endpoint and send the metrics to the desired platform.

![metrics pattern](images/metrics_pattern.png)

#### Datadog

[Datadog open metrics integration](https://docs.datadoghq.com/integrations/openmetrics/)

#### Dynatrace

[Dynatrace Prometheus extension](https://www.dynatrace.com/support/help/extend-dynatrace/extensions20/data-sources/prometheus-extensions)

#### New Relic

[New Relic Prometheus integration](https://docs.newrelic.com/docs/infrastructure/prometheus-integrations/get-started/send-prometheus-metric-data-new-relic/#OpenMetrics)

#### Grafana

This refers to the self managed version of Granfana, not Grafana Cloud.

[Grafana Prometheus datasource](https://grafana.com/docs/grafana/latest/datasources/prometheus/)

## Logs

If you are using `customer metrics`, we will store you logs in your cloud providers blob storage solution.

### Patterns for accessing logs

The general pattern for getting logs from blob storage in to the cloud providers solution is to write a custom serverless function that watches the blob storage and uploads to the desired solution.

![logs pattern](images/logs_pattern.png)

#### AWS

You could leverage some Python code to read the S3 bucket, then use the AWS APIs to upload to cloud watch for example: [aws-load-balancer-logs-to-cloudwatch](https://github.com/rupertbg/aws-load-balancer-logs-to-cloudwatch)

#### Azure

You could leverage azure functions to read the azure storage object, the use API to upload the data to cloud watch.

### Patterns for access logs via a third party provider

If you use a 3rd party observability platform such as Datadog, Dynatrace, or New Relic, a common pattern is to have a serverless function watch blob storage events, and trigger the function to send the logs to the desired platform.

Some platform providers have limitations regarding the ingestion of logs. Read the vendor documentation carefully.

#### Datadog

AWS: [Collecting logs from S3 buckets](https://docs.datadoghq.com/logs/guide/send-aws-services-logs-with-the-datadog-lambda-function/?tab=awsconsole#collecting-logs-from-s3-buckets)

Azure: [Log collection from Blob Storage](https://docs.datadoghq.com/integrations/azure/?tab=blobstorage#create-a-new-azure-blob-storage-function)

#### Dynatrace

AWS: [S3 log forwarder](https://github.com/dynatrace-oss/dynatrace-aws-s3-log-forwarder)

Warning: At the time of writing, the Dynatrace integration will only work if you can stream the logs from Azure Storage to Azure event hub

Azure: [Azure log forwarder](https://github.com/dynatrace-oss/dynatrace-azure-log-forwarder)

#### New Relic

AWS: [Lambda for sending logs from S3](https://docs.newrelic.com/docs/logs/forward-logs/aws-lambda-sending-logs-s3/)

Azure: [Send logs from Azure Blob storage](https://docs.newrelic.com/docs/logs/forward-logs/azure-log-forwarding/#azure-blob-storage)
